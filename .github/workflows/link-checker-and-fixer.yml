name: 链接检查与自动修复

on:
  schedule:
    # 每周一凌晨3点运行
    - cron: '0 3 * * 1'
  # 允许手动触发工作流
  workflow_dispatch:

jobs:
  check-and-fix-links:
    runs-on: ubuntu-latest
    
    steps:
      - name: 检出代码
        uses: actions/checkout@v3
        with:
          # 需要完整的git历史用于创建PR
          fetch-depth: 0
      
      - name: 设置Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: 安装依赖
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 linkchecker

      - name: 检查并整理链接
        id: fix-links
        run: |
          # 创建一个Python脚本来检查和修复链接
          cat > fix_links.py << 'EOL'
          import re
          import requests
          from bs4 import BeautifulSoup
          import time
          import json
          import os
          
          def check_url(url, timeout=10):
              """检查URL是否可访问"""
              try:
                  headers = {
                      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                  }
                  response = requests.head(url, timeout=timeout, headers=headers, allow_redirects=True)
                  if response.status_code >= 400:
                      # 尝试GET请求，有些服务器不接受HEAD
                      response = requests.get(url, timeout=timeout, headers=headers, allow_redirects=True)
                  return response.status_code < 400
              except Exception as e:
                  print(f"检查URL时出错: {url}, 错误: {str(e)}")
                  return False
          
          def check_github_repo(url):
              """检查GitHub仓库是否存在，并尝试查找替代URL"""
              # 提取GitHub仓库路径
              match = re.search(r'github\.com/([^/]+/[^/]+)', url)
              if not match:
                  return None
                  
              repo_path = match.group(1)
              
              # 检查是否有重定向
              try:
                  response = requests.get(f'https://github.com/{repo_path}', 
                                         allow_redirects=False,
                                         headers={'User-Agent': 'Mozilla/5.0'})
                  if 300 <= response.status_code < 400 and 'Location' in response.headers:
                      return response.headers['Location']
              except:
                  pass
                  
              # 检查用户的其他仓库
              try:
                  username = repo_path.split('/')[0]
                  response = requests.get(f'https://github.com/{username}')
                  if response.status_code == 200:
                      # 用户存在，但仓库可能已重命名或删除
                      return f"https://github.com/{username}"
              except:
                  pass
                  
              return None
          
          def process_markdown_file(file_path):
              """处理Markdown文件中的链接"""
              with open(file_path, 'r', encoding='utf-8') as f:
                  content = f.read()
              
              # 使用正则表达式提取所有链接
              url_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
              broken_links = []
              fixed_links = {}
              replacements = []
              
              for match in re.finditer(url_pattern, content):
                  text, url = match.groups()
                  original_url = url
                  
                  # 跳过非http链接
                  if not (url.startswith('http://') or url.startswith('https://')):
                      continue
                  
                  # 清除URL中的查询参数
                  clean_url = url.split('?')[0].split('#')[0]
                  
                  print(f"检查链接: {clean_url}")
                  
                  # 检查链接是否有效
                  if not check_url(clean_url):
                      print(f"发现失效链接: {clean_url}")
                      broken_links.append({
                          'text': text,
                          'url': url,
                          'position': match.start()
                      })
                      
                      # 尝试网页存档
                      archive_url = f"https://web.archive.org/web/{url}"
                      if check_url(archive_url):
                          fixed_links[url] = archive_url
                          replacements.append((match.group(0), f"[{text}]({archive_url}) [存档链接]"))
                          print(f"找到存档链接: {archive_url}")
                      
                      # 如果是GitHub仓库，尝试查找替代URL
                      elif 'github.com' in url:
                          alternative = check_github_repo(url)
                          if alternative and alternative != url:
                              fixed_links[url] = alternative
                              replacements.append((match.group(0), f"[{text}]({alternative}) [可能已迁移]"))
                              print(f"找到替代GitHub链接: {alternative}")
                  
                  # 休眠一小段时间，避免请求过快
                  time.sleep(0.5)
              
              # 应用替换
              new_content = content
              for old, new in replacements:
                  new_content = new_content.replace(old, new)
              
              # 如果有变更，写回文件
              if new_content != content:
                  with open(file_path, 'w', encoding='utf-8') as f:
                      f.write(new_content)
                  print(f"已更新文件 {file_path} 中的失效链接")
              
              # 写入报告
              with open('link_report.json', 'w', encoding='utf-8') as f:
                  json.dump({
                      'broken_links': broken_links,
                      'fixed_links': fixed_links
                  }, f, ensure_ascii=False, indent=2)
              
              # 生成Markdown格式的报告
              with open('link_report.md', 'w', encoding='utf-8') as f:
                  f.write("# 链接检查报告\n\n")
                  
                  if broken_links:
                      f.write("## 失效链接\n\n")
                      for link in broken_links:
                          f.write(f"- [{link['text']}]({link['url']})\n")
                      
                      if fixed_links:
                          f.write("\n## 已修复链接\n\n")
                          for original, fixed in fixed_links.items():
                              f.write(f"- 原链接: {original}\n")
                              f.write(f"  修复为: {fixed}\n\n")
                  else:
                      f.write("恭喜！未发现失效链接。\n")
              
              return len(broken_links) > 0, len(fixed_links) > 0
          
          # 处理README文件
          has_broken_links, has_fixes = process_markdown_file('README.md')
          
          # 设置输出变量
          with open(os.environ['GITHUB_ENV'], 'a') as f:
              f.write(f"HAS_BROKEN_LINKS={'true' if has_broken_links else 'false'}\n")
              f.write(f"HAS_FIXES={'true' if has_fixes else 'false'}\n")
          EOL
          
          # 运行Python脚本
          python fix_links.py
      
      - name: 创建Pull Request
        if: env.HAS_FIXES == 'true'
        run: |
          # 配置Git
          git config --global user.name "LinkFixerBot"
          git config --global user.email "bot@example.com"
          
          # 创建新分支
          git checkout -b fix-broken-links-$(date +%Y%m%d)
          
          # 添加并提交更改
          git add README.md
          git commit -m "自动修复失效链接" || echo "没有变更需要提交"
          
          # 推送更改
          git push origin fix-broken-links-$(date +%Y%m%d)
          
          # 创建PR
          gh pr create --title "修复失效链接" \
                      --body "此PR由链接检查器机器人自动创建，用于修复README中的失效链接。详情请见附加的报告。" \
                      --label "链接维护" || echo "创建PR失败"
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      
      - name: 创建问题(Issue)报告失效链接
        if: env.HAS_BROKEN_LINKS == 'true' && env.HAS_FIXES == 'false'
        uses: peter-evans/create-issue-from-file@v4
        with:
          title: "发现失效链接"
          content-filepath: ./link_report.md
          labels: "链接维护, 需要手动修复" 