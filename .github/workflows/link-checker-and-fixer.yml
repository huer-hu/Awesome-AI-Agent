name: Link Checker and Fixer

on:
  workflow_dispatch:
  schedule:
    # Run at 3 AM every Monday
    - cron: '0 3 * * 1'
  

jobs:
  check-and-fix-links:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v3
        with:
          # Need full git history for PR creation
          fetch-depth: 0
          # Use PAT with write permissions instead of default token
          token: ${{ secrets.REPO_ACCESS_TOKEN }}
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 linkchecker

      - name: Check and Fix Links
        id: fix-links
        run: |
          # Create a Python script to check and fix links
          cat > fix_links.py << 'EOL'
          import re
          import requests
          from bs4 import BeautifulSoup
          import time
          import json
          import os
          
          def check_url(url, timeout=10):
              """Check if URL is accessible"""
              try:
                  headers = {
                      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                  }
                  response = requests.head(url, timeout=timeout, headers=headers, allow_redirects=True)
                  if response.status_code >= 400:
                      # Try GET request, some servers don't accept HEAD
                      response = requests.get(url, timeout=timeout, headers=headers, allow_redirects=True)
                  return response.status_code < 400
              except Exception as e:
                  print(f"Error checking URL: {url}, Error: {str(e)}")
                  return False
          
          def check_github_repo(url):
              """Check if GitHub repo exists and try to find alternative URL"""
              # Extract GitHub repo path
              match = re.search(r'github\.com/([^/]+/[^/]+)', url)
              if not match:
                  return None
                  
              repo_path = match.group(1)
              
              # Check for redirects
              try:
                  response = requests.get(f'https://github.com/{repo_path}', 
                                         allow_redirects=False,
                                         headers={'User-Agent': 'Mozilla/5.0'})
                  if 300 <= response.status_code < 400 and 'Location' in response.headers:
                      return response.headers['Location']
              except:
                  pass
                  
              # Check user's other repositories
              try:
                  username = repo_path.split('/')[0]
                  response = requests.get(f'https://github.com/{username}')
                  if response.status_code == 200:
                      # User exists, but repo may have been renamed or deleted
                      return f"https://github.com/{username}"
              except:
                  pass
                  
              return None
          
          def process_markdown_file(file_path):
              """Process links in Markdown file"""
              with open(file_path, 'r', encoding='utf-8') as f:
                  content = f.read()
              
              # Use regex to extract all links
              url_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
              broken_links = []
              fixed_links = {}
              replacements = []
              
              for match in re.finditer(url_pattern, content):
                  text, url = match.groups()
                  original_url = url
                  
                  # Skip non-http links
                  if not (url.startswith('http://') or url.startswith('https://')):
                      continue
                  
                  # Clean URL of query parameters
                  clean_url = url.split('?')[0].split('#')[0]
                  
                  print(f"Checking link: {clean_url}")
                  
                  # Check if link is valid
                  if not check_url(clean_url):
                      print(f"Found broken link: {clean_url}")
                      broken_links.append({
                          'text': text,
                          'url': url,
                          'position': match.start()
                      })
                      
                      # Try web archive
                      archive_url = f"https://web.archive.org/web/{url}"
                      if check_url(archive_url):
                          fixed_links[url] = archive_url
                          replacements.append((match.group(0), f"[{text}]({archive_url}) [Archived Link]"))
                          print(f"Found archive link: {archive_url}")
                      
                      # If GitHub repo, try to find alternative URL
                      elif 'github.com' in url:
                          alternative = check_github_repo(url)
                          if alternative and alternative != url:
                              fixed_links[url] = alternative
                              replacements.append((match.group(0), f"[{text}]({alternative}) [Possibly Migrated]"))
                              print(f"Found alternative GitHub link: {alternative}")
                  
                  # Sleep briefly to avoid too many requests
                  time.sleep(0.5)
              
              # Apply replacements
              new_content = content
              for old, new in replacements:
                  new_content = new_content.replace(old, new)
              
              # If changes were made, write back to file
              if new_content != content:
                  with open(file_path, 'w', encoding='utf-8') as f:
                      f.write(new_content)
                  print(f"Updated broken links in file {file_path}")
              
              # Write report
              with open('link_report.json', 'w', encoding='utf-8') as f:
                  json.dump({
                      'broken_links': broken_links,
                      'fixed_links': fixed_links
                  }, f, ensure_ascii=False, indent=2)
              
              # Generate Markdown format report
              with open('link_report.md', 'w', encoding='utf-8') as f:
                  f.write("# Link Check Report\n\n")
                  
                  if broken_links:
                      f.write("## Broken Links\n\n")
                      for link in broken_links:
                          f.write(f"- [{link['text']}]({link['url']})\n")
                      
                      if fixed_links:
                          f.write("\n## Fixed Links\n\n")
                          for original, fixed in fixed_links.items():
                              f.write(f"- Original: {original}\n")
                              f.write(f"  Fixed to: {fixed}\n\n")
                  else:
                      f.write("Congratulations! No broken links found.\n")
              
              return len(broken_links) > 0, len(fixed_links) > 0
          
          # Process README file
          has_broken_links, has_fixes = process_markdown_file('README.md')
          
          # Set output variables
          with open(os.environ['GITHUB_ENV'], 'a') as f:
              f.write(f"HAS_BROKEN_LINKS={'true' if has_broken_links else 'false'}\n")
              f.write(f"HAS_FIXES={'true' if has_fixes else 'false'}\n")
          EOL
          
          # Run Python script
          python fix_links.py
      
      - name: Create Pull Request
        if: env.HAS_FIXES == 'true'
        run: |
          # Configure Git
          git config --global user.name "LinkFixerBot"
          git config --global user.email "bot@example.com"
          
          # Create new branch
          git checkout -b fix-broken-links-$(date +%Y%m%d)
          
          # Add and commit changes
          git add README.md
          git commit -m "Automatically fix broken links" || echo "No changes to commit"
          
          # Push changes
          git push origin fix-broken-links-$(date +%Y%m%d)
          
          # Create PR
          gh pr create --title "Fix Broken Links" \
                      --body "This PR was automatically created by the link checker bot to fix broken links in the README. See the attached report for details." \
                      --label "link-maintenance" || echo "Failed to create PR"
        env:
          GITHUB_TOKEN: ${{ secrets.REPO_ACCESS_TOKEN }}
      
      - name: Create Issue to Report Broken Links
        if: env.HAS_BROKEN_LINKS == 'true' && env.HAS_FIXES == 'false'
        uses: peter-evans/create-issue-from-file@v4
        with:
          title: "Broken Links Detected"
          content-filepath: ./link_report.md
          labels: "link-maintenance, manual-fix-needed" 